{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](logo.png \"Logo\")\n",
    "\n",
    "# Módulo 4 - Big Data\n",
    "\n",
    "Autor: Juan Esquivel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivos\n",
    "\n",
    "## General\n",
    "Entender y aplicar técnicas de análisis de grandes cantidades de datos para la resolución de problemas concretos a través de tecnologías de manipulación, extracción y sintetización estadística.\n",
    "\n",
    "## Específicos\n",
    "- Aplicar bibliotecas para la transformación de datos a gran escala para poder sintetizar el conocimiento para futuro análisis.\n",
    "- Aplicar técnicas de análisis de datos para extraer patrones que mejoren el entendimiento de un problema concreto.\n",
    "- Aplicar técnicas para aprendizaje automatizado de patrones, basado en datos existentes, para mejorar la certeza de la solución aplicada a problemas concretos.\n",
    "\n",
    "\n",
    "# Contenidos\n",
    "\n",
    "- Introducción a procesamiento a gran escala\n",
    "- Fuentes y repositorios de datos\n",
    "- Procesamiento de fuentes (data frames)\n",
    "- Procesamiento de atributos\n",
    "- Organización de datos procesados\n",
    "- Análisis de datos a gran escala\n",
    "- Uso de modelos de aprendizaje automático a gran escala\n",
    "\n",
    "# Evaluación\n",
    "\n",
    "- Tareas Cortas 70%\n",
    "- Proyecto 30%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de datos a gran escala\n",
    "Hoy día es relativamente sencillo tener cantidades muy considerables de datos en una organización, sin tan siquiera darse cuenta. El almacenamiento, local o en la nube, ha llegado a un punto que nos permite simplemente almacenar más y más datos sin tener que preocuparse por costos de almacenamiento. Cualquier sistema de ventas, por ejemplo, almacena información diaria sobre clientes, transacciones, tiendas, productos, etc. Después de un par de años de acumular datos, un programa que *funcionaba bien* de pronto empieza a sufrir efectos de degradación de servicio, debido al crecimiento orgánico de los datos.\n",
    "\n",
    "Aún más, es más común que no sea factible tener todos los datos relacionados a un problema en una sola máquina física. Por ello, es necesario particionar los datos y distribuirlos en granjas de servidores de almacenamiento. Algunas definiciones colocan la línea limítrofe de Big Data utilizando esa barrera de almacenamiento distribuído.\n",
    "\n",
    "## Lectura y transformación de datos a gran escala\n",
    "El mayor tamaño de las fuentes de datos requiere que, para obtener información estratégica, se deba implementar elementos de software que permitan procesar y resumir los datos. Normalmente se refiere a ésto como *ETL: Extract, Transform and Load*.\n",
    "\n",
    "Primero, los datos deben *Extraerse* de las fuentes. Después, deben ser *Transformados*, ya sea para crear datos más complejos o ajustar para que sea compatible con el destino. Por último, los datos deben ser *Cargados* a un medio donde serán consumidos por los usuarios.\n",
    "\n",
    "A lo largo de la última década, la necesidad de administrar procesos *ETL* a gran escala se ha convertido en un imperativo para un científico de datos. Esto se realiza, comúnmente, utilizando *frameworks* de alta escalabilidad basados en un modelo de programación llamado *MapReduce*. En esencia, la idea es no pensar sobre los datos como conjuntos enormes de información, sino reducir las operaciones a qué debe realizarse \"fila por fila\". Queremos aplicar operaciones individuales a cada elemento, independiente de cualquier otro en la colección (al menos en teoría). Las operaciones que modifican cada elemento se llaman funciones *map*. Adicionalmente, podemos generar operaciones que sinteticen múltiples elementos que corresponden a la misma entidad reducidendólos a un solo elemento (*reduce*). Plataformas modernas ya no se apegan al espíritu exacto de *MapReduce*, sin embargo, las ideas básicas se mantienen igual.\n",
    "\n",
    "En las siguiente sección introducimos los conceptos clásicos de *MapReduce*. Posteriormente, realizaremos una introducción práctica a frameworks contemporáneos. En este módulo utilizaremos Apache Spark para mostrar los conceptos básicos de este tipo de procesamiento. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivación MapReduce\n",
    "El problema resuelto por *MapReduce* se ilustra con el ejemplo clásico de la frecuencia de palabras en una colección abundante de texto. Si tenemos un corpus compuesto por millones de libros de texto, podríamos pensar en un algoritmo de fuerza bruta que representa cada libro como un arreglo de tipo `string`:\n",
    "```\n",
    "func count(words []string) {\n",
    "  var counts map[string]int\n",
    "  \n",
    "  for _, word:=range words {\n",
    "    counts[word] += 1\n",
    "  }\n",
    "  printSorted(counts)\n",
    "}\n",
    "```\n",
    "\n",
    "En Go el anterior ejemplo puede alcanzar órdenes de magnitudes de centenas de millones, convirtiéndose la memoria en el cuello de botella primario. Es posible que el procesamiento no requiera todas las palabras en memoria, utilizando canales de transmisión (`channel` en Go). Sin embargo, cuando todas las palabras son únicas aún tenemos problemas.\n",
    "\n",
    "Si dejamos los linderos de una máquina individual y asumimos que tenemos, por ejemplo, 10 núcleos de procesamiento. Podríamos dividir el total de los datos en 10 partes, cada núcleo puede generar conteos para cada segmento de datos y, posteriormente, lo envía a un controlador. El controlador procedería a integrar todos los conteos.\n",
    "\n",
    "Si queremos repetir este proceso con 1000 máquinas, un solo controlador colapsaría. Podemos agregar una jerarquía de controladores, sin embargo, donde cada 10 máquina envía a un controlador intermedio y esos, a su vez, a un controlador mayor.\n",
    "\n",
    "Cuando se trabaja con un número tan elevado de núcleos de procesamiento, la probabilidad de que alguna máquina presente errores se incrementa. Si definimos el error que una sola máquina falle como $\\epsilon=0.001$ podemos definir la probabilidad que 10 máquinas ejecuten el proceso sin tener errores como $(1-\\epsilon)^{10}=0.999^{10}=0.9900448802$. Esto quiere decir que en más del 99% de las ejecuciones no debería existir errores en las máquinas y, por lo tanto, la tarea debería concluirse exitosamente. El mismo razonamiento para 1000 máquinas, sin embargo, nos da una probabilidad que todos los núcleos sean exitosos de 37%, únicamente ($(1-\\epsilon)^{1000}=0.999^{1000}=0.3676954248$.\n",
    "\n",
    "Esto quiere deir que el modelado debe incorporar tolerancia a fallos integralemente, replicando las entradas (3 copias), distribuyendo el mismo trabajo entre máquinas no relacionadas entre sí, utilización de checksums, etc.\n",
    "\n",
    "## Detalles de MapReduce\n",
    "De acuerdo a su publicación original *MapReduce* es un modelo de programación para procesar y generar grandes conjuntos de datos. Todo el procesamiento se basa en expresar los datos en pares *llave/valor*. Se debe definir una función de mapeo de los pares originales a una representación intermedia que, posteriormente, es tomada por una función de reducción que une todas las llaves en una sola entrada. Existe una fase oculta a los usuarios que ordena los datos (*shuffle*) previo a la reducción. Si se tienen $R$ máquinas a cargo de la reducción, la fase de shuffle asigna los datos intermedios al nodo con índice `hash(key) % R`.\n",
    "\n",
    "En gran medida, la popularidad de este modelo es que una gran cantidad de problemas reales se pueden modelar de esta forma. Además, los autores crearon un ambiente de ejecución donde los programas escritos en esta forma eran paralelizados y ejecutados de forma distribuida de manera automática. \n",
    "\n",
    "Para el ejemplo del conteo de frecuencias de palabras, el algoritmo para resolverlo sería:\n",
    "\n",
    "```\n",
    "map(String key, String[] value):\n",
    "  // key: document name\n",
    "  // value: document contents\n",
    "  for each word in value:\n",
    "    EmitIntermediate(w, \"1\")\n",
    "\n",
    "reduce(String key, Iterator values):\n",
    "  // key: a word\n",
    "  // values: a list of counts\n",
    "  int result = 0;\n",
    "  for each v in values:\n",
    "    result += ParseInt(v);\n",
    "  Emit(AsString(result));\n",
    "```\n",
    "\n",
    "El siguiente diagrama muestra el proceso completo que efectúa el ambiente de ejecución de *MapReduce*.\n",
    "\n",
    "![mapreduce](mr.png \"Map Reduce\")\n",
    "\n",
    "## Limitaciones de MapReduce\n",
    "Como se mencionó anteriormente, los problemas resueltos por MapReduce *deben* expresarse como una colección de elementos *llave/valor*. De no ser posible, no se puede utilizar el framework para bordarlos.\n",
    "\n",
    "Además, tiene problemas con algoritmos que son iterativos, debido a que en la fase del mapeo se asume que cada elemento es independiente de otros.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos de ejemplo\n",
    "A manera de ejemplo, asumiremos que una tienda en línea almacena las compras de sus clientes en una base de datos PostgreSQL. Aplicaremos ciertas operaciones básicas sobre los datos para mostrar lo que este tipo de plataformas ofrecen. A continuación, un ejemplo básico de modelo de datos, que se usará posteriormente.\n",
    "\n",
    "```sql\n",
    "CREATE TABLE transactions (\n",
    "  id SERIAL PRIMARY KEY,\n",
    "  customer_id integer NOT NULL,\n",
    "  amount integer NOT NULL,\n",
    "  purchased_at timestamp without time zone NOT NULL\n",
    ");\n",
    "\n",
    "\n",
    "INSERT INTO \"transactions\" (customer_id, amount, purchased_at) VALUES\n",
    "(1, 55, '2017-03-01 09:00:00'),\n",
    "(1, 125, '2017-03-01 10:00:00'),\n",
    "(1, 32, '2017-03-02 13:00:00'),\n",
    "(1, 64, '2017-03-02 15:00:00'),\n",
    "(1, 128, '2017-03-03 10:00:00'),\n",
    "(2, 333, '2017-03-01 09:00:00'),\n",
    "(2, 334, '2017-03-01 09:01:00'),\n",
    "(2, 333, '2017-03-01 09:02:00'),\n",
    "(2, 11, '2017-03-03 20:00:00'),\n",
    "(2, 44, '2017-03-03 20:15:00');\n",
    "```\n",
    "\n",
    "# Apache Spark\n",
    "\n",
    "Existen multiples *frameworks* para procesar datos a gran escala. Para el propósito de esta clase, utilizaremos Apache Spark ya que es un proyecto abierto que ha sido adoptado por una gran candidad de desarrolladores y también es un motor primario en servicios en la nube, como Amazon Web Services. \n",
    "\n",
    "Spark está basado en una abstracción llamada *Resilient Distributed Dataset* (RDD) que es una colección de elementos que pueden ser almacenados, temporal o permanentemente, a lo largo de múltuples nodos físicos de un cluster. Spark permite el uso de múltiples fuentes de datos, desde SQL y NoSQL hasta archivos de texto plano. La tarea primaria de un programador es diseñar las operaciones sucesivas par transformar el RDD de su forma original a la salida deseada.\n",
    "\n",
    "El siguiente ejemplo de código utiliza la base de datos descrita anteriormente y muestra el contenido almacenado en ella. Nótese que si la tabla fuera muy grande llamar al método `show()` no es recomendable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark')\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, date_format, udf \n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Basic JDBC pipeline\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/Users⁩/fmezaoba⁩/Documents/Coding⁩/BD⁩/1/postgresql-42.1.4.jar\") \\\n",
    "    .config(\"spark.executor.extraClassPath\", \"/Users⁩/fmezaoba⁩/Documents/Coding⁩/BD⁩/1/postgresql-42.1.4.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Reading single DataFrame in Spark by retrieving all rows from a DB table.\n",
    "df = spark \\\n",
    "    .read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost/fmezaoba\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"bigdata\") \\\n",
    "    .option(\"dbtable\", \"transactions\") \\\n",
    "    .load()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leer datos directamente rara vez es el objetivo primario. Las dos tareas típicas son: transformar algunas de las columnas en representaciones modificadas de las mismas, o bien, agregar grupos de filas en una. A manera de ejemplo, podemos asumir que se quiere la información del total de compras hechas por clientes.\n",
    "\n",
    "Spark provee ciertas transformaciones básicas. Por ejemplo, transformar fechas a una representación con formato específico, para poder truncar la una estampilla de tiempo a nivel de día. En este caso, se utiliza `date_format` en el módulo `pyspark.sql.functions`. El siguiente código crea una columna nueva después de aplicar la transformación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "formatted_df = df.withColumn(\"date_string\", date_format(col(\"purchased_at\"), 'MM/dd/yyyy'))\n",
    "formatted_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso que necesitemos crear una función que no es parte de la biblioteca estándar en Spark, es posible definir funciones creadas por el usuario (User Defined Functions o *udf*). La noción básica de una `udf` en Spark un lambda acompañado por el tipo de dato retornado. Lo anterior es estrictamente necesario en lenguajes con un sistema de tipos débil.\n",
    "\n",
    "Además, es una manera cómoda de encapsular funciones de Python que deben ser aplicadas a celdas del *Dataframe*, pero aún no tienen su implementación en las bibliotecas.\n",
    "\n",
    "El siguiente ejemplo muestra la columna creada previamente (tipo `string`) y la transforma en un tipo `DateType` propio de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "string_to_date = \\\n",
    "    udf(lambda text_date: datetime.strptime(text_date, '%m/%d/%Y'),\n",
    "        DateType())\n",
    "\n",
    "typed_df = formatted_df.withColumn(\"date\", string_to_date(formatted_df.date_string))\n",
    "typed_df.show()\n",
    "typed_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para sumar los datos podemos utilizar el concepto común de agrupamiento en SQL. Spark posee una agrupación de `groupBy`, directamente. En este caso, queremos sumar todas las compras por cliente y día:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_df = typed_df.groupBy(\"customer_id\", \"date\").sum()\n",
    "sum_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark sumará todas las columnas **que no se encuentren** especificadas en la operación `groupBy`. Algunos resultados no tienen interpretación últil. Por ejemplo, sumar la columna `customer_id` no da ningún valor agregado. Finalmente, es posible dar a las columnas un nombre más amigable con los consumidores de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats_df = \\\n",
    "    sum_df.select(\n",
    "        col('customer_id'),\n",
    "        col('date'),\n",
    "        col('sum(amount)').alias('amount'))\n",
    "\n",
    "stats_df.printSchema()\n",
    "stats_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark permite cargar información de múltiples fuentes. A continuación se muestra como cargar datos de un archivo CSV que contiene los nombres de los clientes, así como la moneda en que realizan transacciones. Nótese que el CSV no tiene información de tipos de datos, por lo que es buena práctica agregarlos explícitamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n",
    "\n",
    "names_df = spark \\\n",
    "    .read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"path\", \"names.csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(StructType([\n",
    "                StructField(\"id\", IntegerType()),\n",
    "                StructField(\"name\", StringType()),\n",
    "                StructField(\"currency\", StringType())])) \\\n",
    "    .load()\n",
    "\n",
    "names_df.printSchema()\n",
    "names_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que la información fue cargada, la fuente particular no es relevante. A continuación se muestra como podemos enriquerecer la información utilizando la funcion **JOIN** entre *data frames*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "joint_df = stats_df.join(names_df, stats_df.customer_id == names_df.id)\n",
    "joint_df.printSchema()\n",
    "joint_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio\n",
    "- Cree un pequeño generador de transacciones. Para ello, puede utilizar funciones de numpy o pandas, para crear las transacciones nuevas en memoria y, posteriormente, puede cargarlas a un *Spark Dataframe* que después debe insertar en la base de datos.\n",
    "- Con los datos nuevos, ejecute el código de éste notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuración\n",
    "Para ejecutar el código de ejemplo en este notebook es necesario instalar Jupyter localmente. con soporte de Python. También es necesario instalar una serie de módulos utilizando el comando `pip`:\n",
    "\n",
    "```bash\n",
    "pip3 install pyspark\n",
    "pip3 install findspark\n",
    "```\n",
    "\n",
    "Referencias adicionales: [Spark](https://spark.apache.org/) y [PostgreSQL](https://www.postgresql.org/).\n",
    "\n",
    "# Referencias\n",
    "- Schutt, R; O'Neill C. Doing Data Science - Straight Talk from the Frontline. O'Reilly Media. 2013 (Capítulo 14)\n",
    "- Dean, J; Ghemawat, S. MapReduce: Simplified Data Processing on Large Clusters. https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "colab": {
   "default_view": {},
   "name": "ejemplo_latex.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
